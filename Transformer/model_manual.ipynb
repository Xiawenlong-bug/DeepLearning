{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):#这个类用于将离散的词汇索引（通常是整数）映射到连续的密集向量\n",
    "    def __init__(self,vocab_size,d_model):#d_model是qkv空间的维度\n",
    "        super(TokenEmbedding,self).__init__(vocab_size,d_model,padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self,d_model,maxlen,device):\n",
    "        super(PositionalEmbedding,self).__init__()\n",
    "        self.encoding=torch.zeros(maxlen,d_model,device=device)\n",
    "        self.encoding.requires_grad_(False)\n",
    "\n",
    "        pos=torch.arange(0,maxlen,device=device)\n",
    "        pos=pos.float().unsqueeze(1)#加一维,变成向量\n",
    "        _2i=torch.arange(0,d_model,2,device=device)\n",
    "        self.encoding[:,0::2] = torch.sin(pos/(10000**(_2i/d_model)))#broadcast机制\n",
    "        self.encoding[:,1::2] = torch.cos(pos/(10000**(_2i/d_model)))\n",
    "\n",
    "    def forward(self,x):\n",
    "        seq_leng=x.shape[1]#x 的 形状 ： batch sequenceline dimension\n",
    "        return self.encoding[:seq_leng,:]#选择前seq_leng行，所有列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model,maxlen,dropout,device):\n",
    "        super(TransformerEmbedding,self).__init__()\n",
    "        self.tok_emb=TokenEmbedding(vocab_size,d_model)\n",
    "        self.pos_emb=PositionalEmbedding(d_model,maxlen,device)\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        tok_emb=self.tok_emb(x)\n",
    "        pos_emb=self.pos_emb(x)\n",
    "        return self.drop(tok_emb+pos_emb)#将token嵌入和pos嵌入相加，再通过dropout\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./picture/layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):#图像用BatchNorm比较多 \n",
    "    def __init__(self, d_model, eps=1e-10):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True)\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./picture/ffn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model,hidden,dropout=0.1):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.fc1=nn.Linear(d_model,hidden)\n",
    "        self.fc2=nn.Linear(hidden,d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        feature_map=self.fc1(x)\n",
    "        feature_map=F.relu(feature_map)\n",
    "        feature_map=self.fc2(feature_map)\n",
    "        feature_map=self.dropout(feature_map)#在哪里dropout\n",
    "        return feature_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self,d_model,n_heads,n_groups):\n",
    "        super(GroupQueryAttention,self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.n_heads=n_heads\n",
    "        self.n_groups=n_groups\n",
    "\n",
    "        assert d_model%n_heads == 0\n",
    "        self.n_heads_groups=self.n_heads//self.n_groups#整除操作符\n",
    "        self.head_dim=d_model//self.n_heads\n",
    "\n",
    "        self.w_q=nn.Linear(d_model,d_model)\n",
    "        self.w_l=nn.Linear(d_model,self.n_groups*self.head_dim)#这里的维数\n",
    "        self.w_v=nn.Linear(d_model,self.n_groups*self.head_dim)\n",
    "\n",
    "        self.w_combine=nn.Linear(d_model,d_model)#将多头输出合并为单头输出\n",
    "        self.softmax=nn.Softmax(dim=-1)\n",
    "    \"\"\"\n",
    "    1. 使用`[:,:,None,:,:]`在第二个维度（即头数维度）上添加一个新的维度，大小为1。  \n",
    "    2. 使用`expand`将新维度扩展到`n_groups`和`n_heads_groups`。  \n",
    "    3. 使用`contiguous`确保张量是连续的（这有助于后续的`view`操作）。  \n",
    "    4. 使用`view`改变张量的形状，使其变为`[batch, n_groups * n_heads_groups, time, head_dim]`。\n",
    "    \"\"\"\n",
    "    def expand(self,data):\n",
    "        batch,time=data.shape[0],data.shape[2]\n",
    "        data=data[:,:,None,:,:].expand(batch,self.n_groups,self.n_heads_groups,time,self.head_dim).contiguous()\n",
    "        data=data.view(batch,self.n_groups*self.n_heads_groups,time,self.head_dim)\n",
    "\n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        q=self.w_q(q)\n",
    "        k=self.w_k(k)\n",
    "        v=self.w_v(v)\n",
    "\n",
    "        batch=q.shape[0]    \n",
    "        q=q.view(batch,-1,self.n_groups*self.n_heads_groups,self.head_dim).permute(0,2,1,3)#permute 方法用于重新排列张量的维度。\n",
    "        k=k.view(batch,-1,self.n_groups,self.head_dim).permute(0,2,1,3)\n",
    "        v=v.view(batch,-1,self.n_groups,self.head_dim).permute(0,2,1,3)\n",
    "\n",
    "        k=self.expand(k)\n",
    "        v=self.expand(v)\n",
    "        score=q@k.transpose(2,3)/math.sqrt(self.head_dim)#@是矩阵乘法matmul\n",
    "        if mask is not None:\n",
    "            score=score.masked_fill(mask==0,-1e9)\n",
    "        score=self.softmax(score)@v\n",
    "        #.contiguous 确保张量是连续的（contiguous）在内存中。在 PyTorch 中，当张量经过某些操作（如 transpose、permute 等）后，其内存可能不再是连续的，这可能导致后续的某些操作（如 view）失败。\n",
    "        score=score.permute(0,2,1,3).contiguous().view(batch,-1,self.d_model)\n",
    "        output=self.w_combine(score)\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,n_head):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.w_q = nn.Linear(d_model, d_model)#query\n",
    "        self.w_k = nn.Linear(d_model, d_model)#key\n",
    "        self.w_v = nn.Linear(d_model, d_model)#value\n",
    "        self.w_combine = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch, time, dimension = q.shape\n",
    "        n_d = self.d_model // self.n_head#整除\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "\n",
    "        q = q.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)#通道维度交换\n",
    "        k = k.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)\n",
    "\n",
    "        score = q @ k.transpose(2, 3) / math.sqrt(n_d)\n",
    "        if mask is not None:\n",
    "            # mask = torch.tril(torch.ones(time, time, dtype=bool))\n",
    "            score = score.masked_fill(mask == 0, -1e9)#在softmax时把0的地方设置为负无穷\n",
    "        score = self.softmax(score) @ v\n",
    "\n",
    "        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)#通道维度交换回来    \n",
    "\n",
    "        output = self.w_combine(score)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,n_head,drop_prob)->None:\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.attention=MultiHeadAttention(d_model,n_head)\n",
    "        self.norm1=LayerNorm(d_model)\n",
    "        self.drop1=nn.Dropout(drop_prob)\n",
    "\n",
    "        self.ffn=PositionwiseFeedForward(d_model,ffn_hidden,drop_prob)\n",
    "        self.norm2=LayerNorm(d_model)\n",
    "        self.drop2=nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self,x,mask=None):\n",
    "        _x=x\n",
    "        x=self.attention(x,x,x,mask)\n",
    "        x=self.drop1(x)\n",
    "        x=self.norm1(x+_x)\n",
    "\n",
    "        _x=x\n",
    "        x=self.ffn(x)\n",
    "        x=self.drop2(x)\n",
    "        x=self.norm2(x+_x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,n_head,drop_prob)->None:\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.attention1=MultiHeadAttention(d_model,n_head)\n",
    "        self.norm1=LayerNorm(d_model)\n",
    "        self.drop1=nn.Dropout(drop_prob)\n",
    "\n",
    "        self.attention2=MultiHeadAttention(d_model,n_head)\n",
    "        self.norm2=LayerNorm(d_model)\n",
    "        self.drop2=nn.Dropout(drop_prob)\n",
    "\n",
    "        self.ffn=PositionwiseFeedForward(d_model,ffn_hidden,drop_prob)\n",
    "        self.norm3=LayerNorm(d_model)\n",
    "        self.drop3=nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self,dec,enc,t_mask,s_mask):\n",
    "        _x=dec\n",
    "        x=self.attention1(dec,dec,dec,t_mask)#下三角掩码\n",
    "        x=self.drop1(x)\n",
    "        x=self.norm1(x+_x)\n",
    "\n",
    "        if enc is not None:\n",
    "            _x=x\n",
    "            x=self.attention2(x,enc,enc,s_mask)\n",
    "            x=self.drop2(x)\n",
    "            x=self.norm2(x+_x)\n",
    "        _x=x\n",
    "        x=self.ffn(x)\n",
    "        x=self.drop3(x)\n",
    "        x=self.norm3(x+_x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,enc_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,drop_prob,device):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.embedding=TransformerEmbedding(enc_voc_size,d_model,max_len,drop_prob,device)\n",
    "        #这行代码实际上使用了列表推导式（list comprehension）而不是显式的for循环，但它背后的概念是相同的：重复执行某个操作（在这个例子中是创建EncoderLayer对象）n_layer次。\n",
    "        self.layers=nn.ModuleList([EncoderLayer(d_model,ffn_hidden,n_head,drop_prob)\n",
    "                                   for _ in range(n_layer)\n",
    "                                   ])\n",
    "    def forward(self,x,s_mask):\n",
    "        x=self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,s_mask)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,dec_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,drop_prob,device):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.embedding=TransformerEmbedding(dec_voc_size,d_model,max_len,drop_prob,device)\n",
    "        self.layers=nn.ModuleList([DecoderLayer(d_model,ffn_hidden,n_head,drop_prob)\n",
    "                                   for _ in range(n_layer)\n",
    "                                   ])\n",
    "        self.fc=nn.Linear(d_model,dec_voc_size)\n",
    "\n",
    "    def forward(self,dec,enc,t_mask,s_mask):\n",
    "        dec=self.embedding(dec)\n",
    "        for layer in self.layers:\n",
    "            dec=layer(dec,enc,t_mask,s_mask)\n",
    "        dec=self.fc(dec)\n",
    "\n",
    "        return dec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        enc_voc_size,\n",
    "        dec_voc_size,\n",
    "        max_len,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        ffn_hidden,\n",
    "        n_layers,\n",
    "        drop_prob,\n",
    "        device,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            enc_voc_size,\n",
    "            max_len,\n",
    "            d_model,\n",
    "            ffn_hidden,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            drop_prob,\n",
    "            device,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            dec_voc_size,\n",
    "            max_len,\n",
    "            d_model,\n",
    "            ffn_hidden,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            drop_prob,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "    def make_pad_mask(self,q,k,pad_idx_q,pad_idx_k):\n",
    "        len_q,len_k=q.size(1),k.size(1)#.size和.shape有什么区别\n",
    "#         # (Batch, Time, len_q, len_k)\n",
    "#         * `q.ne(pad_idx_q)`: 检查`q`中的每个元素是否不等于`pad_idx_q`，返回一个布尔张量，其中True表示非填充元素，False表示填充元素。  \n",
    "# * `unsqueeze(1)`和`unsqueeze(3)`: 在第二维和第四维上增加一个维度，使张量的形状从`(Batch, len_q)`变为`(Batch, 1, len_q, 1)`。  \n",
    "# * `repeat(1, 1, 1, len_k)`: 在第四维上重复张量`len_k`次，使其形状变为`(Batch, 1, len_q, len_k)`。\n",
    "        q = q.ne(pad_idx_q).unsqueeze(1).unsqueeze(3)\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "\n",
    "        k = k.ne(pad_idx_k).unsqueeze(1).unsqueeze(2)\n",
    "        k = k.repeat(1, 1, len_q, 1)\n",
    "\n",
    "        mask = q & k\n",
    "        return mask\n",
    "\n",
    "    def make_causal_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        mask = (\n",
    "        # \"\"\"\n",
    "        # * `torch.ones(len_q, len_k)`: 创建一个形状为`(len_q, len_k)`的全1矩阵。  \n",
    "        # * `torch.tril(...)`: 使用`torch.tril`函数，我们保留矩阵的下三角部分（包括对角线），并将其余部分设置为0。在因果掩码中，下三角部分（包括对角线）为True，表示这些位置上的元素在计算自注意力时可以“看到”或“注意”到；而上三角部分为False，表示在计算当前位置的注意力时，不能“看到”未来的位置。  \n",
    "        # * `.type(torch.BoolTensor)`: 将结果矩阵的数据类型转换为布尔类型（True/False）。  \n",
    "        # * `.to(self.device)`: 将掩码移动到与类实例关联的设备上（可能是CPU或某个GPU）。\n",
    "        # \"\"\"\n",
    "            torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)\n",
    "        trg_mask = self.make_pad_mask(\n",
    "            trg, trg, self.trg_pad_idx, self.trg_pad_idx\n",
    "        ) * self.make_causal_mask(trg, trg)#按元素相乘\n",
    "        src_trg_mask = self.make_pad_mask(trg, src, self.trg_pad_idx, self.src_pad_idx)\n",
    "\n",
    "        enc = self.encoder(src, src_mask)\n",
    "        ouput = self.decoder(trg, enc, trg_mask, src_trg_mask)\n",
    "        return ouput\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, \"weight\") and m.weight.dim()>1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11641/198369884.py:3: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6030,  0.0775, -0.2066,  ..., -0.1603,  0.6623,  0.0877],\n",
      "         [-0.9200, -1.3989,  0.9649,  ...,  1.6006,  0.7463, -0.1547],\n",
      "         [-0.4330, -0.9066,  0.5620,  ...,  2.5040,  1.5987,  0.9786],\n",
      "         ...,\n",
      "         [-0.2800, -1.7155,  0.1142,  ...,  1.9078,  1.4097, -0.0481],\n",
      "         [-0.1703, -0.2110,  1.2231,  ...,  1.7769,  2.7302, -0.5516],\n",
      "         [-0.5018, -0.6823,  0.8812,  ...,  3.5247,  2.0960, -0.7109]],\n",
      "\n",
      "        [[ 1.3253, -1.5052,  0.8819,  ...,  0.5924,  2.2748,  0.5442],\n",
      "         [ 1.8348, -2.2479,  0.3603,  ...,  2.0228,  1.9620,  0.5682],\n",
      "         [ 1.0710, -1.0933,  0.7381,  ...,  1.8463,  1.5230, -0.2546],\n",
      "         ...,\n",
      "         [ 1.2449,  0.0946,  0.0158,  ...,  1.9686,  2.2020,  0.4011],\n",
      "         [-0.6390, -0.2855,  2.8089,  ...,  1.4181,  3.3454, -0.8175],\n",
      "         [-0.1937, -0.7905,  0.7057,  ...,  2.4416,  1.2948, -1.4453]],\n",
      "\n",
      "        [[-0.4480, -0.1526,  0.7066,  ...,  1.6856,  2.2949,  0.3683],\n",
      "         [ 0.5991, -0.0410,  0.6618,  ...,  1.1278,  2.7181,  0.1474],\n",
      "         [-0.8878, -0.6759, -0.0403,  ...,  1.5920,  2.6113, -0.5932],\n",
      "         ...,\n",
      "         [-0.9435, -0.4590,  0.8769,  ...,  2.9951,  2.8400, -0.0209],\n",
      "         [-0.7559, -0.5292,  0.7613,  ...,  1.4183,  2.6372, -1.0612],\n",
      "         [ 0.2075,  0.2232,  0.9903,  ...,  1.9030,  2.0187,  0.3829]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.5696, -2.2089,  1.1651,  ...,  3.0888,  1.8830,  0.2721],\n",
      "         [-0.6425, -2.5681,  0.7059,  ...,  2.9776,  2.1310, -0.9826],\n",
      "         [-0.6442, -2.0183,  0.9980,  ...,  2.5067,  2.4195,  0.9326],\n",
      "         ...,\n",
      "         [-1.1953, -0.0363,  0.6638,  ...,  1.7698,  2.5751,  0.0631],\n",
      "         [ 0.8256, -1.7472,  1.3811,  ...,  1.8944,  2.4165,  1.3504],\n",
      "         [-1.0935, -1.3992,  0.7825,  ...,  1.9995,  2.9107,  0.5100]],\n",
      "\n",
      "        [[-0.8019, -1.0534,  0.8412,  ...,  2.9599,  0.9616, -0.6152],\n",
      "         [ 0.6817,  0.3094,  2.2435,  ...,  2.1821,  1.9557, -0.3676],\n",
      "         [ 0.0331,  0.5253,  0.2628,  ...,  2.8420,  2.4878,  0.1225],\n",
      "         ...,\n",
      "         [ 1.1026, -0.6500,  1.8527,  ...,  2.1479,  0.2290,  0.4778],\n",
      "         [-0.3446, -0.1344,  2.0700,  ...,  1.9593,  2.5711,  0.5795],\n",
      "         [-0.3656, -1.1156,  0.5208,  ...,  2.1858,  2.8335,  0.3821]],\n",
      "\n",
      "        [[-1.6474, -0.4244,  1.4404,  ...,  3.4166,  1.8732,  0.1638],\n",
      "         [-0.0666, -1.9506,  1.2529,  ...,  2.1483,  2.6467, -0.1493],\n",
      "         [ 0.7224, -2.4392,  0.3229,  ...,  1.7305,  2.4470, -0.5028],\n",
      "         ...,\n",
      "         [ 0.4103, -0.5623,  1.5883,  ...,  3.0176,  3.0672,  0.0689],\n",
      "         [ 0.0573, -0.9241,  1.3804,  ...,  1.9876,  3.4031, -0.6434],\n",
      "         [ 0.1183, -0.3530,  0.0858,  ...,  2.0189,  3.2714, -0.5386]]],\n",
      "       grad_fn=<ViewBackward0>) torch.Size([128, 38, 7853])\n",
      "------done-------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    enc_voc_size = 5893\n",
    "    dec_voc_size = 7853\n",
    "    src_pad_idx = 1\n",
    "    trg_pad_idx = 1\n",
    "    trg_sos_idx = 2\n",
    "    batch_size = 128\n",
    "    max_len = 1024\n",
    "    d_model = 512\n",
    "    n_layers = 3\n",
    "    n_heads = 2\n",
    "    ffn_hidden = 1024\n",
    "    drop_prob = 0.1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = Transformer(\n",
    "        src_pad_idx=src_pad_idx,\n",
    "        trg_pad_idx=trg_pad_idx,\n",
    "        d_model=d_model,\n",
    "        enc_voc_size=enc_voc_size,\n",
    "        dec_voc_size=dec_voc_size,\n",
    "        max_len=max_len,\n",
    "        ffn_hidden=ffn_hidden,\n",
    "        n_heads=n_heads,\n",
    "        n_layers=n_layers,\n",
    "        drop_prob=drop_prob,\n",
    "        device=device,\n",
    "    ).to(device)\n",
    "\n",
    "    model.apply(initialize_weights)\n",
    "    src = torch.load(\"tensor_src.pt\")\n",
    "    src = torch.cat((src, torch.ones(src.shape[0], 2, dtype=torch.int)), dim=-1)\n",
    "    trg = torch.load(\"tensor_trg.pt\")\n",
    "\n",
    "    result = model(src, trg)\n",
    "    print(result, result.shape)\n",
    "    print(\"------done-------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
