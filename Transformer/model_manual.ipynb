{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):#这个类用于将离散的词汇索引（通常是整数）映射到连续的密集向量\n",
    "    def __init__(self,vocab_size,d_model):#d_model是qkv空间的维度\n",
    "        super(TokenEmbedding,self).__init__(vocab_size,d_model,padding_idx=1)#padding_idx=1不进行梯度更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么区分广播机制\n",
    "写多了广播机制我就分不清维数了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self,d_model,maxlen,device):\n",
    "        super(PositionalEmbedding,self).__init__()\n",
    "        self.encoding=torch.zeros(maxlen,d_model,device=device)\n",
    "        self.encoding.requires_grad_(False)\n",
    "\n",
    "        pos=torch.arange(0,maxlen,device=device)\n",
    "\n",
    "        pos=pos.float().unsqueeze(1)#加一维,变成向量\n",
    "\n",
    "        _2i=torch.arange(0,d_model,2,device=device)\n",
    "\n",
    "        self.encoding[:,0::2] = torch.sin(pos/(10000**(_2i/d_model)))#broadcast机制\n",
    "        #   _2i   (d_model//2, 1 )\n",
    "        #   pos   (maxlen,  1 )\n",
    "        self.encoding[:,1::2] = torch.cos(pos/(10000**(_2i/d_model)))\n",
    "\n",
    "    def forward(self,x):\n",
    "        seq_leng=x.shape[1]#x 的 形状 ： batch sequenceline dimension\n",
    "        return self.encoding[:seq_leng,:]#选择前seq_leng行，所有列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model,maxlen,dropout,device):\n",
    "        super(TransformerEmbedding,self).__init__()\n",
    "        self.tok_emb=TokenEmbedding(vocab_size,d_model)\n",
    "        self.pos_emb=PositionalEmbedding(d_model,maxlen,device)\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        tok_emb=self.tok_emb(x)\n",
    "        pos_emb=self.pos_emb(x)\n",
    "        return self.drop(tok_emb+pos_emb)#将token嵌入和pos嵌入相加，再通过dropout\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./picture/layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):#图像用BatchNorm比较多 \n",
    "    def __init__(self, d_model, eps=1e-10):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True)\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./picture/ffn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model,hidden,dropout=0.1):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.fc1=nn.Linear(d_model,hidden)\n",
    "        self.fc2=nn.Linear(hidden,d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        feature_map=self.fc1(x)\n",
    "        feature_map=F.relu(feature_map)\n",
    "        feature_map=self.fc2(feature_map)\n",
    "        feature_map=self.dropout(feature_map)#在哪里dropout\n",
    "        return feature_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self,d_model,n_heads,n_groups):\n",
    "        super(GroupQueryAttention,self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.n_heads=n_heads\n",
    "        self.n_groups=n_groups\n",
    "\n",
    "        assert d_model%n_heads == 0\n",
    "        self.n_heads_groups=self.n_heads//self.n_groups#整除操作符\n",
    "        self.head_dim=d_model//self.n_heads\n",
    "\n",
    "        self.w_q=nn.Linear(d_model,d_model)\n",
    "        self.w_l=nn.Linear(d_model,self.n_groups*self.head_dim)#这里的维数\n",
    "        self.w_v=nn.Linear(d_model,self.n_groups*self.head_dim)\n",
    "        \n",
    "        self.w_combine=nn.Linear(d_model,d_model)#将多头输出合并为单头输出\n",
    "        self.softmax=nn.Softmax(dim=-1)\n",
    "    \"\"\"\n",
    "        1. 使用`[:,:,None,:,:]`在第二个维度（即头数维度）上添加一个新的维度，大小为1。  \n",
    "        2. 使用`expand`将新维度扩展到`n_groups`和`n_heads_groups`。  \n",
    "        3. 使用`contiguous`确保张量是连续的（这有助于后续的`view`操作）。  \n",
    "        4. 使用`view`改变张量的形状，使其变为`[batch, n_groups * n_heads_groups, time, head_dim]`。\n",
    "    \"\"\"\n",
    "    def expand(self,data):\n",
    "        batch,time=data.shape[0],data.shape[2]\n",
    "        data=data[:,:,None,:,:].expand(batch,self.n_groups,self.n_heads_groups,time,self.head_dim).contiguous()\n",
    "        data=data.view(batch,self.n_groups*self.n_heads_groups,time,self.head_dim)\n",
    "        \n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        q=self.w_q(q)\n",
    "        k=self.w_k(k)\n",
    "        v=self.w_v(v)\n",
    "\n",
    "        batch=q.shape[0]    \n",
    "        q=q.view(batch,-1,self.n_groups*self.n_heads_groups,self.head_dim).permute(0,2,1,3)#permute 方法用于重新排列张量的维度。\n",
    "        k=k.view(batch,-1,self.n_groups,self.head_dim).permute(0,2,1,3)\n",
    "        v=v.view(batch,-1,self.n_groups,self.head_dim).permute(0,2,1,3)\n",
    "\n",
    "        k=self.expand(k)\n",
    "        v=self.expand(v)\n",
    "        score=q@k.transpose(2,3)/math.sqrt(self.head_dim)#@是矩阵乘法matmul\n",
    "        if mask is not None:\n",
    "            score=score.masked_fill(mask==0,-1e9)\n",
    "        score=self.softmax(score)@v\n",
    "        #.contiguous 确保张量是连续的（contiguous）在内存中。在 PyTorch 中，当张量经过某些操作（如 transpose、permute 等）后，其内存可能不再是连续的，这可能导致后续的某些操作（如 view）失败。\n",
    "        score=score.permute(0,2,1,3).contiguous().view(batch,-1,self.d_model)\n",
    "        output=self.w_combine(score)\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为啥要通道交换？\n",
    "这里直接乘的话会有什么后果？\n",
    "\n",
    "n_head是头个数\n",
    "\n",
    "怎么去确定每个矩阵的维度，我感觉写着写着就乱了----根据论文复现代码\n",
    "\n",
    "包括下面的矩阵乘法\n",
    "\n",
    "transpose没搞懂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,n_head):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.w_q = nn.Linear(d_model, d_model)#query\n",
    "        self.w_k = nn.Linear(d_model, d_model)#key\n",
    "        self.w_v = nn.Linear(d_model, d_model)#value\n",
    "        self.w_combine = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch, time, dimension = q.shape\n",
    "        n_d = self.d_model // self.n_head#整除\n",
    "\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        \n",
    "        # q = q.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)#通道维度交换\n",
    "        # k = k.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)\n",
    "        # v = v.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)\n",
    "\n",
    "        # score = q @ k.transpose(2, 3) / math.sqrt(n_d)\n",
    "\n",
    "        score=q@k.transpose()/math.sqrt(n_d)\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask = torch.tril(torch.ones(time, time, dtype=bool))\n",
    "            score = score.masked_fill(mask == 0, -1e9)#在softmax时把0的地方设置为负无穷\n",
    "        score = self.softmax(score) @ v\n",
    "\n",
    "        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)#通道维度交换回来    \n",
    "\n",
    "        output = self.w_combine(score)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,n_head,drop_prob)->None:\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.attention=MultiHeadAttention(d_model,n_head)\n",
    "        self.norm1=LayerNorm(d_model)\n",
    "        self.drop1=nn.Dropout(drop_prob)\n",
    "\n",
    "        self.ffn=PositionwiseFeedForward(d_model,ffn_hidden,drop_prob)\n",
    "        self.norm2=LayerNorm(d_model)\n",
    "        self.drop2=nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self,x,mask=None):\n",
    "        _x=x\n",
    "        x=self.attention(x,x,x,mask)\n",
    "        x=self.drop1(x)\n",
    "        x=self.norm1(x+_x)\n",
    "\n",
    "        _x=x\n",
    "        x=self.ffn(x)\n",
    "        x=self.drop2(x)\n",
    "        x=self.norm2(x+_x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,n_head,drop_prob)->None:\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.attention1=MultiHeadAttention(d_model,n_head)\n",
    "        self.norm1=LayerNorm(d_model)\n",
    "        self.drop1=nn.Dropout(drop_prob)\n",
    "\n",
    "        self.attention2=MultiHeadAttention(d_model,n_head)\n",
    "        self.norm2=LayerNorm(d_model)\n",
    "        self.drop2=nn.Dropout(drop_prob)\n",
    "\n",
    "        self.ffn=PositionwiseFeedForward(d_model,ffn_hidden,drop_prob)\n",
    "        self.norm3=LayerNorm(d_model)\n",
    "        self.drop3=nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self,dec,enc,t_mask,s_mask):\n",
    "        _x=dec\n",
    "        x=self.attention1(dec,dec,dec,t_mask)#下三角掩码\n",
    "        x=self.drop1(x)\n",
    "        x=self.norm1(x+_x)\n",
    "\n",
    "        if enc is not None:\n",
    "            _x=x\n",
    "            x=self.attention2(x,enc,enc,s_mask)\n",
    "            x=self.drop2(x)\n",
    "            x=self.norm2(x+_x)\n",
    "        _x=x\n",
    "        x=self.ffn(x)\n",
    "        x=self.drop3(x)\n",
    "        x=self.norm3(x+_x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "列表推导式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,enc_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,drop_prob,device):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.embedding=TransformerEmbedding(enc_voc_size,d_model,max_len,drop_prob,device)\n",
    "        #这行代码实际上使用了列表推导式（list comprehension）而不是显式的for循环，但它背后的概念是相同的：重复执行某个操作（在这个例子中是创建EncoderLayer对象）n_layer次。\n",
    "        self.layers=nn.ModuleList([EncoderLayer(d_model,ffn_hidden,n_head,drop_prob)\n",
    "                                   for _ in range(n_layer)\n",
    "                                   ])\n",
    "\n",
    "    def forward(self,x,s_mask):\n",
    "        x=self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,s_mask)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,dec_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,drop_prob,device):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.embedding=TransformerEmbedding(dec_voc_size,d_model,max_len,drop_prob,device)\n",
    "        self.layers=nn.ModuleList([DecoderLayer(d_model,ffn_hidden,n_head,drop_prob)\n",
    "                                   for _ in range(n_layer)\n",
    "                                   ])\n",
    "        self.fc=nn.Linear(d_model,dec_voc_size)\n",
    "\n",
    "    def forward(self,dec,enc,t_mask,s_mask):\n",
    "        dec=self.embedding(dec)\n",
    "        for layer in self.layers:\n",
    "            dec=layer(dec,enc,t_mask,s_mask)\n",
    "        dec=self.fc(dec)\n",
    "\n",
    "        return dec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        enc_voc_size,\n",
    "        dec_voc_size,\n",
    "        max_len,\n",
    "        d_model,\n",
    "        n_heads,\n",
    "        ffn_hidden,\n",
    "        n_layers,\n",
    "        drop_prob,\n",
    "        device,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            enc_voc_size,\n",
    "            max_len,\n",
    "            d_model,\n",
    "            ffn_hidden,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            drop_prob,\n",
    "            device,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            dec_voc_size,\n",
    "            max_len,\n",
    "            d_model,\n",
    "            ffn_hidden,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            drop_prob,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "    def make_pad_mask(self,q,k,pad_idx_q,pad_idx_k):\n",
    "        len_q,len_k=q.size(1),k.size(1)#.size和.shape有什么区别\n",
    "#         # (Batch, Time, len_q, len_k)\n",
    "#         * `q.ne(pad_idx_q)`: 检查`q`中的每个元素是否不等于`pad_idx_q`，返回一个布尔张量，其中True表示非填充元素，False表示填充元素。  \n",
    "# * `unsqueeze(1)`和`unsqueeze(3)`: 在第二维和第四维上增加一个维度，使张量的形状从`(Batch, len_q)`变为`(Batch, 1, len_q, 1)`。  \n",
    "# * `repeat(1, 1, 1, len_k)`: 在第四维上重复张量`len_k`次，使其形状变为`(Batch, 1, len_q, len_k)`。\n",
    "        q = q.ne(pad_idx_q).unsqueeze(1).unsqueeze(3)\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "\n",
    "        k = k.ne(pad_idx_k).unsqueeze(1).unsqueeze(2)\n",
    "        k = k.repeat(1, 1, len_q, 1)\n",
    "\n",
    "        mask = q & k\n",
    "        return mask\n",
    "\n",
    "    def make_causal_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        mask = (\n",
    "        # \"\"\"\n",
    "        # * `torch.ones(len_q, len_k)`: 创建一个形状为`(len_q, len_k)`的全1矩阵。  \n",
    "        # * `torch.tril(...)`: 使用`torch.tril`函数，我们保留矩阵的下三角部分（包括对角线），并将其余部分设置为0。在因果掩码中，下三角部分（包括对角线）为True，表示这些位置上的元素在计算自注意力时可以“看到”或“注意”到；而上三角部分为False，表示在计算当前位置的注意力时，不能“看到”未来的位置。  \n",
    "        # * `.type(torch.BoolTensor)`: 将结果矩阵的数据类型转换为布尔类型（True/False）。  \n",
    "        # * `.to(self.device)`: 将掩码移动到与类实例关联的设备上（可能是CPU或某个GPU）。\n",
    "        # \"\"\"\n",
    "            torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)\n",
    "        trg_mask = self.make_pad_mask(\n",
    "            trg, trg, self.trg_pad_idx, self.trg_pad_idx\n",
    "        ) * self.make_causal_mask(trg, trg)#按元素相乘\n",
    "        src_trg_mask = self.make_pad_mask(trg, src, self.trg_pad_idx, self.src_pad_idx)\n",
    "\n",
    "        enc = self.encoder(src, src_mask)\n",
    "        ouput = self.decoder(trg, enc, trg_mask, src_trg_mask)\n",
    "        return ouput\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hasattr是什么，是哪里import进来的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, \"weight\") and m.weight.dim()>1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3954/198369884.py:3: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0182,  0.8573,  2.1676,  ..., -1.0181, -0.8391,  0.6301],\n",
      "         [-0.1038,  0.8671,  2.4428,  ...,  0.0572,  0.7169,  2.1889],\n",
      "         [-0.7449,  0.9061,  2.1292,  ..., -0.6728, -0.5952,  0.2645],\n",
      "         ...,\n",
      "         [-0.0817,  1.5581,  2.5416,  ..., -0.0180, -1.6679, -0.5903],\n",
      "         [-0.3085,  1.6497,  1.8682,  ...,  1.1634, -0.6081,  1.2884],\n",
      "         [-0.6122,  1.1131,  1.8740,  ...,  1.6652, -0.4030,  0.0614]],\n",
      "\n",
      "        [[-0.2470,  1.4090,  1.5266,  ..., -0.3441, -1.0733, -0.0614],\n",
      "         [ 0.5967,  2.1597,  2.0489,  ..., -0.5584,  0.2949,  0.1743],\n",
      "         [ 0.1965,  1.4333,  3.0105,  ...,  0.1107,  0.6705, -0.1335],\n",
      "         ...,\n",
      "         [-1.9299,  2.6737,  2.8277,  ...,  0.1166,  0.6657, -0.1451],\n",
      "         [ 0.3026,  1.4098,  1.8824,  ...,  0.6737,  1.4850,  0.1671],\n",
      "         [-0.4802,  1.9959,  1.8069,  ..., -0.3137,  0.6142, -0.6528]],\n",
      "\n",
      "        [[-0.1369,  1.4830,  2.1619,  ..., -0.1178, -0.1747, -0.3410],\n",
      "         [-0.1441,  1.7695,  3.0233,  ..., -0.2471, -0.5670, -0.7649],\n",
      "         [-0.3631,  0.5421,  2.6288,  ...,  0.5458,  0.6626,  0.4290],\n",
      "         ...,\n",
      "         [ 0.4314,  3.5272,  2.5015,  ...,  1.3266,  0.4793, -1.1621],\n",
      "         [ 0.6655, -0.4160,  0.5367,  ..., -0.2613,  0.1087,  0.0744],\n",
      "         [ 0.7436,  1.5755,  1.3911,  ...,  0.0727,  0.5908,  0.8103]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1073,  1.8621,  3.0145,  ...,  1.1253,  0.5286, -0.3128],\n",
      "         [ 0.1625,  1.6831,  2.5454,  ..., -0.2805,  0.3062, -0.3397],\n",
      "         [ 0.7255,  1.9565,  2.5993,  ...,  0.1908, -0.8515, -0.9502],\n",
      "         ...,\n",
      "         [ 0.0395,  0.8570,  2.2770,  ...,  0.1466,  0.5223,  0.7842],\n",
      "         [ 1.5300,  2.6117,  1.9712,  ...,  2.2198, -0.1164,  0.0911],\n",
      "         [ 0.6816,  0.6801,  1.3962,  ...,  0.8717, -0.0137,  0.6393]],\n",
      "\n",
      "        [[ 2.4094,  0.5690,  2.9782,  ..., -0.4164, -0.8661,  0.2622],\n",
      "         [ 0.6107,  1.9737,  2.6697,  ...,  1.1230,  0.5798,  0.4348],\n",
      "         [-0.7152,  1.5369,  3.5139,  ..., -0.0274,  0.8697,  1.9325],\n",
      "         ...,\n",
      "         [-0.5548,  1.9923,  1.6397,  ..., -0.7167,  1.3262, -0.5312],\n",
      "         [ 0.2125,  1.6007,  2.9547,  ..., -0.9937, -0.0575, -0.2897],\n",
      "         [-0.0938,  0.7849,  2.0702,  ...,  0.1145, -1.0900,  0.0830]],\n",
      "\n",
      "        [[ 0.7773,  1.1453,  1.5856,  ...,  0.5364,  0.0546,  0.6623],\n",
      "         [ 0.6802,  0.8062,  1.5759,  ..., -0.7173, -0.9115, -0.1353],\n",
      "         [-0.6606,  1.3918,  2.2628,  ...,  0.2363, -0.2768, -0.3238],\n",
      "         ...,\n",
      "         [ 0.7006,  0.9347,  1.6794,  ..., -0.6100, -0.3136,  0.0703],\n",
      "         [-0.7385,  0.8533,  2.3702,  ..., -0.1739,  0.4627, -0.7334],\n",
      "         [-0.3076,  1.4693,  2.2931,  ..., -0.5920, -0.8457, -0.3827]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "above is result\n",
      "torch.Size([128, 38, 7853])\n",
      "------done-------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    enc_voc_size = 5893\n",
    "    dec_voc_size = 7853\n",
    "    src_pad_idx = 1\n",
    "    trg_pad_idx = 1\n",
    "    trg_sos_idx = 2\n",
    "    batch_size = 128\n",
    "    max_len = 1024\n",
    "    d_model = 512\n",
    "    n_layers = 3\n",
    "    n_heads = 2\n",
    "    ffn_hidden = 1024\n",
    "    drop_prob = 0.1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = Transformer(\n",
    "        src_pad_idx=src_pad_idx,\n",
    "        trg_pad_idx=trg_pad_idx,\n",
    "        d_model=d_model,\n",
    "        enc_voc_size=enc_voc_size,\n",
    "        dec_voc_size=dec_voc_size,\n",
    "        max_len=max_len,\n",
    "        ffn_hidden=ffn_hidden,\n",
    "        n_heads=n_heads,\n",
    "        n_layers=n_layers,\n",
    "        drop_prob=drop_prob,\n",
    "        device=device,\n",
    "    ).to(device)\n",
    "\n",
    "    model.apply(initialize_weights)\n",
    "    src = torch.load(\"tensor_src.pt\")\n",
    "    src = torch.cat((src, torch.ones(src.shape[0], 2, dtype=torch.int)), dim=-1)\n",
    "    trg = torch.load(\"tensor_trg.pt\")\n",
    "\n",
    "    result = model(src, trg)\n",
    "    print(result)\n",
    "    print(\"above is result\")\n",
    "    print(result.shape)\n",
    "    print(\"------done-------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
